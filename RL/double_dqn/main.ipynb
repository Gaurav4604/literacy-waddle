{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2df0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "env = gym.make(\"CarRacing-v3\", continuous=True)\n",
    "\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f69428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple to store each state, action taken, future state based on this action and the reward for action taken\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e63b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((84, 84)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def get_processed_state(state, device):\n",
    "    # 1. Transform the state\n",
    "    # transform expects (C, H, W) or (H, W, C) depending on version, \n",
    "    # but ToPILImage handles (H, W, C) correctly.\n",
    "    state_t = transform(state).to(device)\n",
    "    \n",
    "    # 2. Add Batch Dimension: (3, 84, 84) -> (1, 3, 84, 84)\n",
    "    state_t = state_t.unsqueeze(0)\n",
    "    \n",
    "    return state_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49442baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7a7fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimeLimit' object has no attribute 'acion_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m TAU \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.005\u001b[39m\n\u001b[0;32m      7\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-4\u001b[39m\n\u001b[1;32m----> 9\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macion_space\u001b[49m\u001b[38;5;241m.\u001b[39mn  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     10\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     12\u001b[0m n_observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(state)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TimeLimit' object has no attribute 'acion_space'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 2500\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "\n",
    "n_actions = env.action_space.n  # type: ignore\n",
    "state, info = env.reset()\n",
    "\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = ConvDQN(n_actions).to(device)\n",
    "target_net = ConvDQN(n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "        -1 * steps_done / EPS_DECAY\n",
    "    )\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(\n",
    "            [[env.action_space.sample()]], device=device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "episode_durations = []\n",
    "episode_losses = []\n",
    "episode_qs = []\n",
    "episode_epsilons = []\n",
    "\n",
    "\n",
    "def plot_training_metrics(show_result=False):\n",
    "    # Create a 2x2 grid\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Rewards\n",
    "    axs[0, 0].plot(episode_rewards, label=\"Total Reward\", color=\"blue\", alpha=0.6)\n",
    "    if len(episode_rewards) >= 50:\n",
    "        means = np.convolve(episode_rewards, np.ones(50) / 50, mode=\"valid\")\n",
    "        axs[0, 0].plot(\n",
    "            range(49, len(episode_rewards)), means, color=\"red\", label=\"50-Ep Avg\"\n",
    "        )\n",
    "    axs[0, 0].set_title(\"Reward per Episode\")\n",
    "    axs[0, 0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "    # 2. Loss\n",
    "    axs[0, 1].plot(episode_losses, color=\"orange\")\n",
    "    axs[0, 1].set_title(\"Average Training Loss per Episode\")\n",
    "    axs[0, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "    # 3. Epsilon\n",
    "    axs[1, 0].plot(episode_epsilons, color=\"green\")\n",
    "    axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "    axs[1, 0].set_ylabel(\"Epsilon\")\n",
    "\n",
    "    # 4. Average Q-Value\n",
    "    axs[1, 1].plot(episode_qs, color=\"purple\")\n",
    "    axs[1, 1].set_title(\"Average Q-Value Estimation\")\n",
    "    axs[1, 1].set_ylabel(\"Q-Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    # Close the figure to prevent memory leaks in loops\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return None, None\n",
    "\n",
    "    # sampled values for experience to look up on\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # converting them to the right data type\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # extracting booleans telling if the current state had next state or not\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "\n",
    "    # extracting the values that had next state, and not worrying about values that didn't\n",
    "    # (since game over states would contribute towards learning process by saying nothing needs to be done, i.e \"Zero next steps needed\")\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # programmatically populating the actual next state rewards by saying\n",
    "        # if there was a next state -> predict the outcome based on current next state\n",
    "        # if there wasn't a next state -> leave the value as Zero (since this is already generated by torch.zeros)\n",
    "        next_state_actions = policy_net(non_final_next_states).max(1).indices.unsqueeze(1)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_state_actions).squeeze(1)\n",
    "\n",
    "\n",
    "    # provide the total reward generated based on weighing of current + future reward\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # calculating loss between the two to update the policy network\n",
    "    # DQN updates the policy network by estimating the loss it has, over itself (unstable for complex scenarios)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return loss and Q-values for plotting (as per previous updates)\n",
    "    return loss.item(), state_action_values.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    raw_state, info = env.reset()\n",
    "    state = get_processed_state(raw_state, device)\n",
    "    \n",
    "    # Variables to track WITHIN the episode\n",
    "    current_reward = 0\n",
    "    temp_losses = []\n",
    "    temp_qs = []\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        raw_next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Process the NEXT state\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = get_processed_state(raw_next_state, device)\n",
    "\n",
    "\n",
    "        # Track reward\n",
    "        current_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        # Capture returns from optimize_model\n",
    "        loss_val, q_val = optimize_model()\n",
    "\n",
    "        # Only append if optimization actually happened (i.e. not None)\n",
    "        if loss_val is not None:\n",
    "            temp_losses.append(loss_val)\n",
    "            temp_qs.append(q_val)\n",
    "\n",
    "        # Soft Update Target Net\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[\n",
    "                key\n",
    "            ] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # 1. Save Metrics\n",
    "            episode_durations.append(t + 1)\n",
    "            episode_rewards.append(current_reward)\n",
    "\n",
    "            # Calculate average loss/Q for this episode\n",
    "            avg_loss = np.mean(temp_losses) if temp_losses else 0\n",
    "            avg_q = np.mean(temp_qs) if temp_qs else 0\n",
    "            episode_losses.append(avg_loss)\n",
    "            episode_qs.append(avg_q)\n",
    "\n",
    "            # Calculate current Epsilon\n",
    "            curr_eps = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "                -1 * steps_done / EPS_DECAY\n",
    "            )\n",
    "            episode_epsilons.append(curr_eps)\n",
    "\n",
    "            # 2. Plot\n",
    "            plot_training_metrics()\n",
    "            break\n",
    "\n",
    "print(\"Done\")\n",
    "plot_training_metrics(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16770ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"racingcar_pytorch_dqn_policy.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
