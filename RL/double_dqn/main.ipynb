{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2df0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "env = gym.make(\"CarRacing-v3\", continuous=False)\n",
    "\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f69428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple to store each state, action taken, future state based on this action and the reward for action taken\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e63b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToPILImage(), T.Resize((84, 84)), T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_processed_state(state, device):\n",
    "    # 1. Transform the state\n",
    "    # transform expects (C, H, W) or (H, W, C) depending on version,\n",
    "    # but ToPILImage handles (H, W, C) correctly.\n",
    "    state_t = transform(state).to(device)\n",
    "\n",
    "    # 2. Add Batch Dimension: (3, 84, 84) -> (1, 3, 84, 84)\n",
    "    state_t = state_t.unsqueeze(0)\n",
    "\n",
    "    return state_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49442baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(ConvDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c7a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 2500\n",
    "TAU = 0.005\n",
    "LR = 3e-4\n",
    "\n",
    "n_actions = env.action_space.n  # type: ignore\n",
    "state, info = env.reset()\n",
    "\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = ConvDQN(n_actions).to(device)\n",
    "target_net = ConvDQN(n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "        -1 * steps_done / EPS_DECAY\n",
    "    )\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(\n",
    "            [[env.action_space.sample()]], device=device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "episode_durations = []\n",
    "episode_losses = []\n",
    "episode_qs = []\n",
    "episode_epsilons = []\n",
    "\n",
    "\n",
    "def plot_training_metrics(show_result=False):\n",
    "    # Create a 2x2 grid\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Rewards\n",
    "    axs[0, 0].plot(episode_rewards, label=\"Total Reward\", color=\"blue\", alpha=0.6)\n",
    "    if len(episode_rewards) >= 50:\n",
    "        means = np.convolve(episode_rewards, np.ones(50) / 50, mode=\"valid\")\n",
    "        axs[0, 0].plot(\n",
    "            range(49, len(episode_rewards)), means, color=\"red\", label=\"50-Ep Avg\"\n",
    "        )\n",
    "    axs[0, 0].set_title(\"Reward per Episode\")\n",
    "    axs[0, 0].set_ylabel(\"Total Reward\")\n",
    "\n",
    "    # 2. Loss\n",
    "    axs[0, 1].plot(episode_losses, color=\"orange\")\n",
    "    axs[0, 1].set_title(\"Average Training Loss per Episode\")\n",
    "    axs[0, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "    # 3. Epsilon\n",
    "    axs[1, 0].plot(episode_epsilons, color=\"green\")\n",
    "    axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "    axs[1, 0].set_ylabel(\"Epsilon\")\n",
    "\n",
    "    # 4. Average Q-Value\n",
    "    axs[1, 1].plot(episode_qs, color=\"purple\")\n",
    "    axs[1, 1].set_title(\"Average Q-Value Estimation\")\n",
    "    axs[1, 1].set_ylabel(\"Q-Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    # Close the figure to prevent memory leaks in loops\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return None, None\n",
    "\n",
    "    # sampled values for experience to look up on\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # converting them to the right data type\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # extracting booleans telling if the current state had next state or not\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "\n",
    "    # extracting the values that had next state, and not worrying about values that didn't\n",
    "    # (since game over states would contribute towards learning process by saying nothing needs to be done, i.e \"Zero next steps needed\")\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # programmatically populating the actual next state rewards by saying\n",
    "        # if there was a next state -> predict the outcome based on current next state\n",
    "        # if there wasn't a next state -> leave the value as Zero (since this is already generated by torch.zeros)\n",
    "        next_state_actions = (\n",
    "            policy_net(non_final_next_states).max(1).indices.unsqueeze(1)\n",
    "        )\n",
    "        next_state_values[non_final_mask] = (\n",
    "            target_net(non_final_next_states).gather(1, next_state_actions).squeeze(1)\n",
    "        )\n",
    "\n",
    "    # provide the total reward generated based on weighing of current + future reward\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # calculating loss between the two to update the policy network\n",
    "    # DQN updates the policy network by estimating the loss it has, over itself (unstable for complex scenarios)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return loss and Q-values for plotting (as per previous updates)\n",
    "    return loss.item(), state_action_values.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402d6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Capture returns from optimize_model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m loss_val, q_val \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Only append if optimization actually happened (i.e. not None)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(state_action_values, expected_state_action_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(policy_net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32me:\\Programming\\Research\\MissionPhD\\Learning\\env\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Programming\\Research\\MissionPhD\\Learning\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Programming\\Research\\MissionPhD\\Learning\\env\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    raw_state, info = env.reset()\n",
    "    state = get_processed_state(raw_state, device)\n",
    "\n",
    "    # Variables to track WITHIN the episode\n",
    "    current_reward = 0\n",
    "    temp_losses = []\n",
    "    temp_qs = []\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        raw_next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Process the NEXT state\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = get_processed_state(raw_next_state, device)\n",
    "\n",
    "        # Track reward\n",
    "        current_reward += reward\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        # Capture returns from optimize_model\n",
    "        loss_val, q_val = optimize_model()\n",
    "\n",
    "        # Only append if optimization actually happened (i.e. not None)\n",
    "        if loss_val is not None:\n",
    "            temp_losses.append(loss_val)\n",
    "            temp_qs.append(q_val)\n",
    "\n",
    "        # Soft Update Target Net\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[\n",
    "                key\n",
    "            ] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # 1. Save Metrics\n",
    "            episode_durations.append(t + 1)\n",
    "            episode_rewards.append(current_reward)\n",
    "\n",
    "            # Calculate average loss/Q for this episode\n",
    "            avg_loss = np.mean(temp_losses) if temp_losses else 0\n",
    "            avg_q = np.mean(temp_qs) if temp_qs else 0\n",
    "            episode_losses.append(avg_loss)\n",
    "            episode_qs.append(avg_q)\n",
    "\n",
    "            # Calculate current Epsilon\n",
    "            curr_eps = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "                -1 * steps_done / EPS_DECAY\n",
    "            )\n",
    "            episode_epsilons.append(curr_eps)\n",
    "\n",
    "            # 2. Plot\n",
    "            plot_training_metrics()\n",
    "            break\n",
    "\n",
    "print(\"Done\")\n",
    "plot_training_metrics(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16770ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"racingcar_pytorch_dqn_policy.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
